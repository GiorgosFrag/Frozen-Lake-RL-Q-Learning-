{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import time \n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the environment\n",
    "env = gym.make('CartPole-v1', render_mode = 'human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The game has continuous observation values so we need to overcome \n",
    "#this problem in order to manufacture the Q Table\n",
    "#we are gonna build bins (classes) for each observation\n",
    "\n",
    "def create_bins(number_of_bins_per_observation= 10):\n",
    "    bins_cart_position = np.linspace(-4.8,4.8,number_of_bins_per_observation)\n",
    "    bins_cart_velocity = np.linspace(-5,5,number_of_bins_per_observation)\n",
    "    bins_pole_angle = np.linspace(-0.418,0.418,number_of_bins_per_observation)\n",
    "    bins_pole_angular_velocity = np.linspace(-5,5,number_of_bins_per_observation)\n",
    "    \n",
    "    bins = np.array([bins_cart_position,bins_cart_velocity, bins_pole_angle, bins_pole_angular_velocity])\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIN_NUM = 10\n",
    "BINS = create_bins(BIN_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.8       , -3.73333333, -2.66666667, -1.6       , -0.53333333,\n",
       "         0.53333333,  1.6       ,  2.66666667,  3.73333333,  4.8       ],\n",
       "       [-5.        , -3.88888889, -2.77777778, -1.66666667, -0.55555556,\n",
       "         0.55555556,  1.66666667,  2.77777778,  3.88888889,  5.        ],\n",
       "       [-0.418     , -0.32511111, -0.23222222, -0.13933333, -0.04644444,\n",
       "         0.04644444,  0.13933333,  0.23222222,  0.32511111,  0.418     ],\n",
       "       [-5.        , -3.88888889, -2.77777778, -1.66666667, -0.55555556,\n",
       "         0.55555556,  1.66666667,  2.77777778,  3.88888889,  5.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 10, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializing the q_table\n",
    "q_table_shape = (BIN_NUM, BIN_NUM, BIN_NUM, BIN_NUM, env.action_space.n)\n",
    "q_table = np.zeros(q_table_shape)\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to classify each observation into each bin \n",
    "#in order to discretize the observasion space\n",
    "def discetize_observation(observations, bins):\n",
    "    bined_observations = []\n",
    "    \n",
    "    for i, observation in enumerate(observations):\n",
    "        discretized_observation = np.digitize(observation, bin[i])#we classify each observation to the apropreate bin\n",
    "        bined_observations.append(discretized_observation)\n",
    "    return tuple(bined_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the hyperparamiters of the algorithm\n",
    "ALPHA = .9 #The Learning Rate\n",
    "GAMMA = .95 #Dicount Factor\n",
    "EPOCHS = 10000 #How many games the agent will try\n",
    "\n",
    "#Exploration vs Exploitation parameters\n",
    "epsilon = 1 #Exploration Rate\n",
    "MAX_EPSILON = 1 #Maximum probabilty of exploration\n",
    "MIN_EPSILON = .01 #Minimum exploration probability\n",
    "DECAY_RATE = .001 #Exponential reduction Rate for the probability of exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action_selection(state, q_table, epsilon):\n",
    "    #set a random cutof for the exploration \n",
    "    random = np.random.random()\n",
    "    \n",
    "    if epsilon < random:\n",
    "        #Exploit! Use the best Q(s,a) from the Q Table\n",
    "        possible_actions = q_table[state]\n",
    "        action = np.argmax(possible_actions) #return the best action\n",
    "    else:\n",
    "        #Explore! Perform a random action from the action space\n",
    "        action = env.action_space.saple()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_value(old_q_value, next_opt_q_value, reward):\n",
    "    new_q_value = old_q_value + ALPHA*(reward + GAMMA*next_opt_q_value - old_q_value)\n",
    "    return new_q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epslon_reduction(epsilon, epoch):\n",
    "    new_epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON)*np.exp(-DECAY_RATE*epoch)\n",
    "    return new_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Algorithm\n",
    "rewards = []\n",
    "\n",
    "\n",
    "for episode in range(EPOCHS):\n",
    "    initial_state = env.reset()[0]\n",
    "    discretized_initial_state = discetize_observation(initial_state, BINS)\n",
    "    \n",
    "    terminate = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not terminate:\n",
    "        #choose an action \n",
    "        action = greedy_action_selection(discretized_initial_state, q_table, epsilon)\n",
    "        #Perform the action\n",
    "        next_state, reward, terminate, truncate, info = env.step(action)\n",
    "        #discretize the next state\n",
    "        discretized_next_state = discetize_observation(next_state, BINS)\n",
    "        \n",
    "        #retrieving the old q value\n",
    "        old_q_value = q_table[discretized_initial_state +(action,)]\n",
    "        \n",
    "        #get the next optimal q_value\n",
    "        next_opt_q_value = np.max(q_table[discretized_next_state])\n",
    "        \n",
    "        #Calculating the new q value\n",
    "        new_q_value = compute_q_value(old_q_value, next_opt_q_value, reward)\n",
    "        \n",
    "        #Updating the Q Table\n",
    "        q_table[discretized_initial_state + (action,)] = new_q_value\n",
    "        #the current(next) state is now the initial state\n",
    "        discretized_initial_state = discretized_next_state\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "    #Reduce the epsilon \n",
    "    epoch = episode +1\n",
    "    epsilon = epslon_reduction(epsilon, epoch)\n",
    "    rewards.append(total_rewards)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the agent to play the game acting on the Q Table\n",
    "\n",
    "state = env.reset()[0]\n",
    "\n",
    "for i in range(100): #Maximum steps the agent allow to do and not winning the game \n",
    "    env.render()\n",
    "    initial_disc_state = discetize_observation(state, BINS)\n",
    "    action = np.argmax(q_table[initial_disc_state])\n",
    "    state, reward, terminate, truncate, info = env.step(action)\n",
    "    \n",
    "    if terminate:\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
